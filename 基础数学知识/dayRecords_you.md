## 2025-2.26
### 随机梯度下降，梯度下降，小批量梯度下降

今天根据李沐的《动手写深度学习》中 MBGD，修改代码，实现BGD发现：

- **学习率的调整** ：对于BGD，损失函数计算的是所有样本的损失，所以loss对w的偏导值会很大，如果沿用MBGD的lr，会发现loss无法收敛，必须降低lr。
- **MBGD的w更新**：对于每一轮epoch，都需要计算一次grad，然后只更新一次，计算grad开销实在太大，而MBGD对小批量的样本计算grad则开销小得多，且每一轮循环可以多次更新w。lr 为 1 算正常，不需要较多轮次便可完成训练


- **矩阵运算 torch.matmul()**：pytorch中的一维数组是以列向量为数学计算约定，而以行向量为表示形式的向量 ,因为此时的size 为 [m] 的tensor 实际上是 列向量