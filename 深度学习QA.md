## 1.为什么深度学习优化算法基本选择的都是 小批量随机梯度下降？
1.因为对于一整个神经网络，我要计算所有的grad，开销非常大，但是每次只更新一个权重，事实上很那通过一次更新就得到最优权重，所以可以把整个dataset划分成很多个小份，这样每个小份都能更新一次，并且计算小份的所有grad开销小，更好的到达loss极值    
2.假设loss存在多个局部最优点，每次采用随机的样本会导致计算出来的grad不一定是顺着loss的下降方向，这样就可能会跳出局部最优点（此处存疑）


## 2.BGD批量梯度下降 训练多少轮更新多少次参数？
是的，训练多少轮更新多少次参数。
SGD是每次使用一个样本作为数据集，所以训练n轮，dataset有m个样本，那么更新次数就是n*m次。  
BGD是用所有个样本作为数据集，更新一次权重，所以训练n轮，dataset有m个样本，那么更新次数就是n次。

## 3.svm从理论上讲对于分类效果很好，和神经网络相比，缺点在哪？
当数据点是一个特别大的量时，svm计算比较困难

## 4.PyTorch 使用注意事项
在pytorch里面，默认只能是标量对标量，或者标量对向量/矩阵求导！
要想使x支持求导，必须让x为浮点类型.

## 5.卷积和池化的异同
1.卷积和池化都有一些超参数，padding 和 stride
2.卷积操作可以指定有多少个卷积核，即将输入通道最后变为n(卷积核个数)个输出通道，但是池化输入通道数=输出通道数

## 6.是不是不同的激活函数一般用的lr不一样？目前来看sigmoid的导数最大为0.25，而relu正区间导数为1，区别挺大，特别是在MLP中，需要用多个激活函数
